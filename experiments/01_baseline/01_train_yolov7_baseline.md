# Experiment 01 — Baseline YOLOv7 (Training + Testing)

This experiment establishes the baseline detector performance used throughout the project.

The goal of this run is to fine-tune YOLOv7 on the SeaDronesSee dataset using the standard training setup, without preprocessing, architectural changes, or post-processing stages.  
The trained model is then evaluated using YOLOv7’s built-in testing procedure.

This baseline serves as the reference point for all subsequent experiments.

---

## Training

The baseline model is trained using the standard YOLOv7 training pipeline.

The command below is executed from the YOLOv7 root directory:

```bash
python3 train.py \
  --workers 4 \
  --device 0 \
  --batch-size 8 \
  --data data/seadronessee.yaml \
  --img 640 640 \
  --cfg cfg/training/yolov7.yaml \
  --weights weights/yolov7.pt \
  --name seadronessee_baseline \
  --hyp data/hyp.scratch.p5.yaml
Training Notes
The model is initialized from the official pretrained YOLOv7 weights.

Default YOLOv7 hyperparameters are used.

No preprocessing, tiling, or class filtering is applied in this experiment.

The same dataset split and class definitions are reused in all subsequent experiments.

Testing (Evaluation)
After training is completed, the best checkpoint is evaluated on the validation split.

Since ground-truth labels are not available for the official SeaDronesSee test set, all quantitative evaluation in this project is performed on the validation split.

The evaluation command is executed from the YOLOv7 root directory:

python3 test.py \
  --weights runs/train/seadronessee_baseline/weights/best.pt \
  --data data/seadronessee.yaml \
  --img 640 \
  --device 0
Outputs
YOLOv7 outputs are written to:

runs/train/seadronessee_baseline/
This directory includes:

Training logs

Intermediate checkpoints

weights/best.pt

weights/last.pt

Evaluation outputs generated by test.py

Due to size constraints, these outputs are not included in the repository.

Summary
This experiment provides a clean baseline for evaluating the impact of:

Data-centric preprocessing for small-object detection

Post-detection CNN-based verification

Size-based (small vs large) performance analysis

All subsequent experiments are compared against this baseline configuration.

